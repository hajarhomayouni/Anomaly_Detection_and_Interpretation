{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detection and Interpretation from Tabular Data\n",
        "\n",
        "## Anomaly Detector"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, AdamW\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import time"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2024-08-09 19:27:18.437742: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-08-09 19:27:19.023548: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-08-09 19:27:23.329491: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-08-09 19:27:23.345078: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-08-09 19:27:27.212095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723231658367
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "file_path = 'ecoli.csv'\n",
        "# Make sure to drop ID column if any\n",
        "df = pd.read_csv(file_path).drop('ID', axis=1)\n",
        "\n",
        "def row_to_sentence(row):\n",
        "    return ','.join([f'[{col}:{val}]' for col, val in row.items()])\n",
        "\n",
        "# Assuming the label column is called 'Class'\n",
        "df['sentence'] = df.drop('Class', axis=1).apply(row_to_sentence, axis=1)\n",
        "sentences = df['sentence'].tolist()\n",
        "\n",
        "df['Class'].value_counts().shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "(8,)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723231658539
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating vocab\n",
        "def generate_vocab(df):\n",
        "    vocab = {}\n",
        "    column_values = {}\n",
        "    for col in df.columns:\n",
        "        if col !='sentence':\n",
        "            unique_values = set(df[col].unique())\n",
        "            column_values[col] = {str(val): None for val in unique_values}  # Convert values to strings\n",
        "\n",
        "    # Assign token IDs\n",
        "    token_id = 0\n",
        "    for col, values in column_values.items():\n",
        "        for val in values:\n",
        "            token = f'[{col}:{val}]'\n",
        "            vocab[token] = token_id\n",
        "            token_id += 1\n",
        "\n",
        "    # Special tokens\n",
        "    vocab[\"<OOV>\"] = token_id\n",
        "    vocab[\"<PAD>\"] = token_id + 1\n",
        "\n",
        "    return vocab, column_values"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723231685466
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Tokenizer based on the closest method\n",
        "def find_closest_known_token(col_val_pair, column_values, threshold=0.5):\n",
        "    col, val = col_val_pair.strip('[]').split(':')\n",
        "    val = float(val)  # Convert value to float for comparison\n",
        "    \n",
        "    closest_val = None\n",
        "    closest_diff = float('inf')\n",
        "    for known_val in column_values[col]:\n",
        "        known_val_float = float(known_val)\n",
        "        diff = abs(known_val_float - val)\n",
        "        if diff < closest_diff and diff <= threshold:\n",
        "            closest_val = known_val\n",
        "            closest_diff = diff\n",
        "\n",
        "    if closest_val is not None:\n",
        "        return f'[{col}:{closest_val}]'\n",
        "    else:\n",
        "        return \"<OOV>\"\n",
        "\n",
        "def is_number(string):\n",
        "    try:\n",
        "        float(string)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def custom_tokenizer(examples, return_tensors=\"pt\"):\n",
        "    tokenized_outputs = []\n",
        "    numerical_values = []\n",
        "\n",
        "    for sentence in examples:\n",
        "        tokens = sentence.split(',')\n",
        "        token_ids = []\n",
        "        nums = []\n",
        "        for idx, token in enumerate(tokens):\n",
        "            if token in vocab:\n",
        "                token_id = vocab[token]\n",
        "            else:\n",
        "                # Handle OOV tokens\n",
        "                closest_token = find_closest_known_token(token, column_values)\n",
        "                token_id = vocab.get(closest_token, vocab[\"<OOV>\"])\n",
        "            if is_number(token.split(':')[1].strip(']')):  # Check if the token contains a number\n",
        "                nums.append((idx, float(token.split(':')[1].strip(']'))))\n",
        "            token_ids.append(token_id)\n",
        "        tokenized_outputs.append(token_ids)\n",
        "        numerical_values.append(nums)\n",
        "\n",
        "    # Compatibility with `datasets` library\n",
        "    return {\"input_ids\": tokenized_outputs, \"numerical_values\": numerical_values}\n",
        "\n",
        "\n",
        "# Generate vocab and column values\n",
        "vocab, column_values = generate_vocab(df)\n",
        "# Constructing id_to_token by reversing vocab\n",
        "id_to_token = {id: token for token, id in vocab.items()}\n",
        "\n",
        "def convert_ids_to_tokens(token_ids):\n",
        "    return [id_to_token.get(id, \"<OOV>\") for id in token_ids]\n",
        "\n",
        "# Test tokenizer\n",
        "text = df['sentence'].iloc[5]\n",
        "print(text)\n",
        "\n",
        "# Tokenize the text\n",
        "inputs = custom_tokenizer([text])\n",
        "\n",
        "# Inspect the tokens to see the effect of whole word masking\n",
        "tokens = convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "print(tokens)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[MCG:0.67],[GVH:0.39],[LIP:0.48],[CHG:0.5],[AAC:0.36],[ALM1:0.38],[ALM2:0.46]\n['[MCG:0.67]', '[GVH:0.39]', '[LIP:0.48]', '[CHG:0.5]', '[AAC:0.36]', '[ALM1:0.38]', '[ALM2:0.46]']\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723231693834
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare for training\n",
        "# Assuming df['Class'] contains labels and df['sentence'] contains the text\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(df['sentence'], df['Class'], test_size=0.1, random_state=42)\n",
        "\n",
        "train_dataset = Dataset.from_dict({'text': train_sentences.tolist(), 'labels': train_labels.tolist()})\n",
        "val_dataset = Dataset.from_dict({'text': val_sentences.tolist(), 'labels': val_labels.tolist()})\n",
        "\n",
        "# Tokenize the datasets\n",
        "train_dataset = train_dataset.map(lambda examples: custom_tokenizer(examples['text']), batched=True)\n",
        "val_dataset = val_dataset.map(lambda examples: custom_tokenizer(examples['text']), batched=True)\n",
        "\n",
        "# The collate_fn function is used in the DataLoader to specify how a list of data samples should be merged into a single batch during the training process. \n",
        "def collate_fn(batch):\n",
        "    max_len = max(len(item['input_ids']) for item in batch)\n",
        "    input_ids = torch.tensor([item['input_ids'] + [vocab[\"<PAD>\"]] * (max_len - len(item['input_ids'])) for item in batch])\n",
        "    labels = torch.tensor([item['labels'] for item in batch])\n",
        "\n",
        "    # Prepare padded numerical values and indices\n",
        "    padded_numerical_values = []\n",
        "    padded_num_indices = []\n",
        "    for item in batch:\n",
        "        nums = item['numerical_values']\n",
        "        indices = [idx for idx, _ in nums]\n",
        "        values = [val for _, val in nums]\n",
        "        padded_num_indices.append(indices + [-1] * (max_len - len(indices)))  # Pad with -1 to ignore during multiplication\n",
        "        padded_numerical_values.append(values + [0] * (max_len - len(values)))  # Pad with 0\n",
        "    \n",
        "    numerical_values = torch.tensor(padded_numerical_values, dtype=torch.float)\n",
        "    num_indices = torch.tensor(padded_num_indices, dtype=torch.long)\n",
        "\n",
        "    return {'input_ids': input_ids, 'labels': labels, 'numerical_values': numerical_values, 'num_indices': num_indices, 'max_len': max_len}\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize custom embeddings\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 768  # Adjust as needed\n",
        "custom_embeddings = torch.FloatTensor(np.random.rand(vocab_size, embedding_dim))\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)  # If using CUDA\n",
        "\n",
        "# Convert custom embeddings to a FloatTensor\n",
        "custom_embeddings_tensor = torch.tensor(custom_embeddings, dtype=torch.float)\n",
        "\n",
        "# Load the model\n",
        "model = BertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2, output_attentions=True)\n",
        "\n",
        "# Replace the model's word embeddings with the custom embeddings\n",
        "model.bert.embeddings.word_embeddings = nn.Embedding(num_embeddings=custom_embeddings_tensor.size(0), embedding_dim=custom_embeddings_tensor.size(1))\n",
        "model.bert.embeddings.word_embeddings.weight = nn.Parameter(custom_embeddings_tensor)\n",
        "\n",
        "# Optionally, freeze the custom embeddings to prevent them from being updated during training\n",
        "# model.bert.embeddings.word_embeddings.weight.requires_grad = False\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Parameter 'function'=<function <lambda> at 0x7f34b6924040> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7dbf77d727d470ea553ca463e070725"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "730b0fa13a70482e94a9f9b17a28bfc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_4420/2605091068.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  custom_embeddings_tensor = torch.tensor(custom_embeddings, dtype=torch.float)\nYou are using a model of type albert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\nSome weights of the model checkpoint at albert-base-v2 were not used when initializing BertForSequenceClassification: ['albert.pooler.weight', 'predictions.bias', 'albert.pooler.bias', 'albert.embeddings.LayerNorm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias', 'predictions.LayerNorm.bias', 'albert.encoder.embedding_hidden_mapping_in.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight', 'albert.embeddings.token_type_embeddings.weight', 'albert.embeddings.position_embeddings.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias', 'albert.embeddings.LayerNorm.bias', 'predictions.decoder.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight', 'predictions.decoder.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias', 'predictions.LayerNorm.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight', 'predictions.dense.weight', 'albert.encoder.embedding_hidden_mapping_in.bias', 'albert.embeddings.word_embeddings.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight', 'predictions.dense.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.intermediate.dense.weight', 'classifier.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.weight', 'classifier.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.3.attention.self.query.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.self.value.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/anaconda/envs/azureml_py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723231710129
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "start_time = time.time()\n",
        "model.train()\n",
        "for epoch in range(3):  # number of epochs\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        numerical_values = batch['numerical_values'].to(device)\n",
        "        num_indices = batch['num_indices'].to(device)\n",
        "        max_len = batch['max_len']\n",
        "        \n",
        "        # Get the embeddings for the input_ids\n",
        "        embeddings = model.bert.embeddings.word_embeddings(input_ids)\n",
        "        \n",
        "        # Multiply the embeddings by the actual numerical values\n",
        "        for batch_idx in range(input_ids.size(0)):\n",
        "            for token_idx in range(max_len):\n",
        "                if num_indices[batch_idx, token_idx] != -1:  # Ignore padded indices\n",
        "                    embeddings[batch_idx, token_idx] *= numerical_values[batch_idx, token_idx]\n",
        "\n",
        "        # Recompute the logits with the updated embeddings\n",
        "        model_output = model(inputs_embeds=embeddings, labels=labels)\n",
        "        loss = model_output.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    print(f\"Epoch {epoch+1} finished. Total Loss: {total_loss}\")\n",
        "training_time=time.time() - start_time"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896330182
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "total_eval_accuracy = 0\n",
        "all_true_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "for batch in tqdm(val_loader):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    numerical_values = batch['numerical_values'].to(device)\n",
        "    num_indices = batch['num_indices'].to(device)\n",
        "    max_len = batch['max_len']\n",
        "\n",
        "    # Get the embeddings for the input_ids\n",
        "    embeddings = model.bert.embeddings.word_embeddings(input_ids)\n",
        "    \n",
        "    # Multiply the embeddings by the actual numerical values\n",
        "    for batch_idx in range(input_ids.size(0)):\n",
        "        for token_idx in range(max_len):\n",
        "            if num_indices[batch_idx, token_idx] != -1:  # Ignore padded indices\n",
        "                embeddings[batch_idx, token_idx] *= numerical_values[batch_idx, token_idx]\n",
        "\n",
        "    # Recompute the logits with the updated embeddings\n",
        "    with torch.no_grad():\n",
        "        model_output = model(inputs_embeds=embeddings, labels=labels)\n",
        "        logits = model_output.logits\n",
        "\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    accuracy = (predictions == labels).cpu().numpy().mean()  # Simplified accuracy calculation\n",
        "    total_eval_accuracy += accuracy\n",
        "\n",
        "    all_true_labels.extend(labels.cpu().numpy())\n",
        "    all_predictions.extend(predictions.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy and F1 score\n",
        "bert_accuracy = total_eval_accuracy / len(val_loader)\n",
        "bert_f1 = f1_score(all_true_labels, all_predictions, average='binary')  # Use 'binary' for binary classification\n",
        "\n",
        "print(f\"Validation Accuracy: {bert_accuracy}\")\n",
        "print(f\"Validation F1 Score: {bert_f1}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896330320
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anomaly Interpreter"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def get_prediction_attention(sentence):\n",
        "    # Wrap the sentence in a list to match the expected input of custom_tokenizer\n",
        "    inputs = custom_tokenizer([sentence])  # Pass a list with a single sentence\n",
        "    \n",
        "    # Convert list of token IDs to a tensor and add a batch dimension\n",
        "    input_ids = torch.tensor(inputs['input_ids'], dtype=torch.long).to(device)\n",
        "    \n",
        "    # Create an attention mask for the input_ids\n",
        "    attention_mask = torch.tensor([[1]*len(input_ids[0])], dtype=torch.long).to(device)\n",
        "    \n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # Model forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_attentions=True)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    attentions = outputs.attentions\n",
        "\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    predicted_class = torch.argmax(probs, dim=-1).cpu().numpy()[0]  # Unwrap batch dimension\n",
        "    predicted_prob = probs[0, predicted_class].item()  # Adjusted for batch\n",
        "\n",
        "    return predicted_class, predicted_prob, attentions\n",
        "\n",
        "# Example usage:\n",
        "example_sentence = df['sentence'].iloc[5]  \n",
        "prediction, probability, attention_weights = get_prediction_attention(example_sentence)\n",
        "print(\"Prediction:\", prediction)\n",
        "print(\"Probability:\", probability)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896330507
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def visualize_attention(input_text, attentions):\n",
        "    # Tokenize the input text using the custom tokenizer\n",
        "    # Assume `custom_tokenizer` returns token IDs for a batch of sentences\n",
        "    tokenized_input = custom_tokenizer([input_text])['input_ids'][0]  # Process a single sentence\n",
        "    \n",
        "    # Convert token IDs back to tokens using `id_to_token`\n",
        "    tokens = [id_to_token[id] for id in tokenized_input]\n",
        "    \n",
        "    # Assuming attentions is a list of tensors, one for each layer\n",
        "    # Get the attentions for the last layer and mean across heads\n",
        "    attention = attentions[-1].squeeze(0).mean(0)  # [seq_len, seq_len]\n",
        "    \n",
        "    # Convert to numpy for visualization\n",
        "    attention_np = attention.cpu().detach().numpy()\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(attention_np, xticklabels=tokens, yticklabels=tokens, cmap='viridis')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "# Assuming `attention_weights` is obtained from the `get_prediction_attention` function\n",
        "visualize_attention(example_sentence, attention_weights)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896330633
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "#Create Association Matrix from Training Data\n",
        "# An association matrix M can capture the average attention between all pairs of words across the training data. \n",
        "# This function iterates over training sentences, calculate attention, and then aggregate these to form M.\n",
        "\n",
        "\n",
        "def create_full_association_matrix(sentences, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Create an association matrix for a subset of tokens based on attention weights across all sentences.\n",
        "    \n",
        "    Args:\n",
        "    sentences (list of str): List of sentences to process.\n",
        "    model (transformers model): Pre-trained transformer model.\n",
        "    tokenizer (transformers tokenizer): Custom tokenizer for the model.\n",
        "    \n",
        "    Returns:\n",
        "    np.ndarray: An association matrix capturing the average association between token pairs.\n",
        "    \"\"\"\n",
        "    token_counts = defaultdict(int)\n",
        "    association_matrix = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n",
        "    \n",
        "    model.eval()  # Ensure model is in evaluation mode\n",
        "    model.to(device)  # Ensure model is on the correct device\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        # Using custom tokenizer here\n",
        "        inputs = tokenizer([sentence])  # Assuming tokenizer processes a list of sentences\n",
        "        input_ids = inputs[\"input_ids\"][0]  # Assuming the tokenizer returns 'input_ids' directly\n",
        "        \n",
        "        # Convert token IDs to a tensor, add batch dimension, and move to the device\n",
        "        input_ids_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids_tensor, output_attentions=True)\n",
        "        attention = outputs.attentions[-1].squeeze(0).mean(dim=0)  # Get last layer's attention, average over all heads\n",
        "        \n",
        "        for i, id1 in enumerate(input_ids):\n",
        "            for j, id2 in enumerate(input_ids):\n",
        "                if id1 < vocab_size and id2 < vocab_size:  # Ensure indices are within the vocab size\n",
        "                    token_counts[id1] += 1\n",
        "                    token_counts[id2] += 1\n",
        "                    # Increment association matrix by attention value, ensuring conversion to scalar with `.item()`\n",
        "                    association_matrix[id1, id2] += attention[i, j].item()\n",
        "    \n",
        "    # Normalize the association scores\n",
        "    for i in range(vocab_size):\n",
        "        for j in range(vocab_size):\n",
        "            total_occurrences = token_counts[i] + token_counts[j]\n",
        "            if total_occurrences > 0:  # Avoid division by zero\n",
        "                association_matrix[i, j] /= total_occurrences\n",
        "    \n",
        "    return association_matrix\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896330760
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are creating association matrix only for normal (i.e., class = 0) training data\n",
        "normal_df=df[df['Class']==0]\n",
        "normal_sentences = normal_df['sentence'].tolist()\n",
        "\n",
        "start_time = time.time()\n",
        "M = create_full_association_matrix(normal_sentences, model, custom_tokenizer)\n",
        "time_to_build_M=time.time() - start_time"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896330894
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the association Matrix, specially the distribution of values\n",
        "plt.hist(M.flatten(), bins=50, log=True)\n",
        "plt.title('Distribution of Association Scores')\n",
        "plt.xlabel('Association Score')\n",
        "plt.ylabel('Frequency (log scale)')\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896331011
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set threshold value for associated words\n",
        "threshold = np.mean(M.flatten())#np.percentile(M.flatten(), 75)\n",
        "violation_threshold=threshold#np.percentile(M.flatten(), 75)\n",
        "print(violation_threshold)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896331131
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Identify Associated Words\n",
        "def identify_high_attention_pairs(attention_matrix, tokens):\n",
        "    high_attention_pairs = []\n",
        "    seq_len = attention_matrix.shape[0]\n",
        "    \n",
        "    for i in range(seq_len):\n",
        "        for j in range(i + 1, seq_len):  # Consider only pairs without repetition and self-attention\n",
        "            if attention_matrix[i, j] > threshold:  # Correct indexing for numpy arrays\n",
        "                token1, token2 = tokens[i], tokens[j]\n",
        "                if token1 not in ['[PAD]', '[SEP]', '[CLS]'] and token2 not in ['[PAD]', '[SEP]', '[CLS]']:  # Filter out special tokens if present\n",
        "                    high_attention_pairs.append((token1, token2))\n",
        "    \n",
        "    return high_attention_pairs"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896331248
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_test_sentence(sentence):\n",
        "    # Assuming 'model', 'device', and 'custom_tokenizer' are defined globally\n",
        "    predicted_class, predicted_prob, attentions = get_prediction_attention(sentence)\n",
        "    # Convert token IDs back to tokens\n",
        "    inputs = custom_tokenizer([sentence])\n",
        "    token_ids = inputs['input_ids'][0]\n",
        "    tokens = [id_to_token.get(id, \"<UNK>\") for id in token_ids]  # Assuming 'id_to_token' mapping exists globally\n",
        "    \n",
        "    if predicted_class == 0 and not (\"<OOV>\" in tokens):\n",
        "        return \"normal\", []\n",
        "\n",
        "    # Flatten attentions from the last layer and average across heads\n",
        "    attention_matrix = attentions[-1].squeeze(0).mean(dim=0).cpu().numpy()\n",
        "    \n",
        "\n",
        "    \n",
        "    high_attention_pairs = identify_high_attention_pairs(attention_matrix, tokens)\n",
        "    \n",
        "    return \"anomalous\", high_attention_pairs\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896331373
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_association_violations(high_attention_pairs, association_matrix, violation_threshold=violation_threshold):\n",
        "    violations = []\n",
        "    # Adjust matrix_size to consider the additional <OOV> and <PAD> tokens\n",
        "    matrix_size = vocab_size\n",
        "\n",
        "    for word1, word2 in high_attention_pairs:\n",
        "        id1 = vocab.get(word1, vocab[\"<OOV>\"])  # Use <OOV> ID for unknown tokens\n",
        "        id2 = vocab.get(word2, vocab[\"<OOV>\"])\n",
        "\n",
        "        # Ensure both IDs are within the bounds of the association matrix\n",
        "        if id1 < matrix_size and id2 < matrix_size:\n",
        "            if association_matrix[id1, id2] < violation_threshold or word1 == \"<OOV>\" or word2 == \"<OOV>\":\n",
        "                violations.append((word1, word2))\n",
        "        else:\n",
        "            # This should not happen given the adjustment for <OOV> and <PAD>, but left for safety\n",
        "            print(f\"Skipping pair ({word1}, {word2}) with IDs ({id1}, {id2}) outside association matrix bounds.\")\n",
        "\n",
        "    return violations\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896331488
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Test\n",
        "example_sentence = df['sentence'].iloc[15]\n",
        "print(example_sentence)\n",
        "\n",
        "status, high_attention_pairs = process_test_sentence(example_sentence)\n",
        "print(f\"Status: {status}\")\n",
        "if status == \"anomalous\":\n",
        "    print(\"Highly associated words based on attention weights:\", high_attention_pairs)\n",
        "    # Assuming M is your association matrix and it's already initialized\n",
        "    violations = check_association_violations(high_attention_pairs, M)\n",
        "    if violations:\n",
        "        print(\"Violations of association rules:\", violations)\n",
        "    else:\n",
        "        print(\"No violations of association rules.\")\n",
        "else:\n",
        "    print(\"Sentence is normal.\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896331604
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mutation Analysis\n",
        "To perform mutation analysis on the dataset and calculate the mutation score, we follow these steps. The code will select 5% of the normal data, randomly mutate two column values per row, and then check if these mutations lead to the data being classified as anomalous and whether the mutated pairs are in the set of violations. The mutation score is calculated as the ratio of mutants killed (those that result in anomalies with violations) to the total number of mutants.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Assuming df is your DataFrame and M is your association matrix\n",
        "\n",
        "# Select 5% of normal data\n",
        "normal_data = df[df['Class'] == 0].sample(frac=0.05)\n",
        "\n",
        "def mutate_sentence(sentence):\n",
        "    columns = sentence.split('],[')\n",
        "    if columns:\n",
        "        columns[0] = columns[0][1:]  # Remove the first '['\n",
        "        columns[-1] = columns[-1][:-1]  # Remove the last ']'\n",
        "    \n",
        "    mutation_indices = random.sample(range(len(columns)), 2)\n",
        "    mutated_columns = []  # Track mutated columns\n",
        "    \n",
        "    for idx in mutation_indices:\n",
        "        col_val_split = columns[idx].split(':')\n",
        "        col = col_val_split[0]\n",
        "        mutated_val = str(random.randint(1,2))\n",
        "        \n",
        "\n",
        "        columns[idx] = f\"{col}:{mutated_val}\"\n",
        "        mutated_columns.append(f\"[{col}:{mutated_val}]\")  # Store mutated column for comparison\n",
        "    \n",
        "    mutated_sentence = '[' + '],['.join(columns) + ']'\n",
        "    return mutated_sentence, mutated_columns\n",
        "\n",
        "killed_mutants_1 = 0\n",
        "killed_mutants_2 = 0\n",
        "total_mutants = len(normal_data)  # Correct total_mutants calculation\n",
        "\n",
        "for sentence in normal_data['sentence']:\n",
        "    mutated_sentence, mutated_columns = mutate_sentence(sentence)\n",
        "\n",
        "    status, high_attention_pairs = process_test_sentence(mutated_sentence)\n",
        "    print(f\"Status: {status}\")\n",
        "    if status == \"anomalous\":\n",
        "        print(\"Highly associated words based on attention:\", high_attention_pairs)\n",
        "        killed_mutants_1 += 1\n",
        "        violations = check_association_violations(high_attention_pairs, M)\n",
        "        if violations:\n",
        "            print(\"Violations of association rules found:\", violations)\n",
        "            # Check if any mutated column is in the violations\n",
        "            if any(\"<OOV>\" in pair for violation in violations for pair in violation) or any(col in pair for violation in violations for pair in violation for col in mutated_columns):\n",
        "               killed_mutants_2 += 1\n",
        "\n",
        "        else:\n",
        "            print(\"No violations of association rules.\")\n",
        "    else:\n",
        "        print(\"Sentence is normal.\")\n",
        "\n",
        "mutation_score_1 = (killed_mutants_1 / total_mutants) * 100\n",
        "mutation_score_2 = (killed_mutants_2 / total_mutants) * 100\n",
        "print(f\"Mutation Score for anomaly detection: {mutation_score_1:.2f}%\")\n",
        "print(f\"Mutation Score for violation detection: {mutation_score_2:.2f}%\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896331728
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare with other models"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Assume the same data preprocessing steps as before, resulting in train_sentences and train_labels\n",
        "\n",
        "# Tokenize the text for LSTM\n",
        "max_words = 5000  # This is the size of the vocabulary\n",
        "max_len = 50  # This should be adjusted to the length of the longest sentence after tokenization\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Define the LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "lstm_model.add(LSTM(64))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the LSTM model\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "lstm_model.fit(padded_sequences, train_labels, batch_size=32, epochs=10, validation_split=0.1, callbacks=[early_stopping])\n",
        "\n",
        "# Prepare the validation data\n",
        "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
        "val_padded_sequences = pad_sequences(val_sequences, maxlen=max_len)\n",
        "val_labels = np.array(val_labels)\n",
        "\n",
        "lstm_preds = (lstm_model.predict(val_padded_sequences) > 0.5).astype('int32').flatten()\n",
        "\n",
        "lstm_accuracy = accuracy_score(val_labels, lstm_preds)\n",
        "lstm_f1 = f1_score(val_labels, lstm_preds)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896331853
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Assuming df is your DataFrame and the last column 'Class' is the label\n",
        "\n",
        "# Separate features and labels\n",
        "X = df.drop(['Class','sentence'], axis=1).values\n",
        "y = df['Class'].values\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_standardized = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into a training and a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_standardized, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Define the MLP model\n",
        "mlp_model = Sequential()\n",
        "mlp_model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "mlp_model.add(Dropout(0.5))\n",
        "mlp_model.add(Dense(32, activation='relu'))\n",
        "mlp_model.add(Dropout(0.5))\n",
        "mlp_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the MLP model with early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "mlp_model.fit(X_train, y_train, batch_size=32, epochs=100, validation_split=0.1, callbacks=[early_stopping])\n",
        "\n",
        "# Make predictions with the MLP model\n",
        "mlp_preds = (mlp_model.predict(X_val) > 0.5).astype('int32').flatten()\n",
        "\n",
        "# Evaluate the MLP model\n",
        "mlp_accuracy = accuracy_score(y_val, mlp_preds)\n",
        "mlp_f1 = f1_score(y_val, mlp_preds)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896331975
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary of Evaluation Results"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(file_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896332095
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compare the models\n",
        "comparison_dict = {\n",
        "    'Model': ['BERT', 'LSTM','MLP'],\n",
        "    'Accuracy': [bert_accuracy, lstm_accuracy,mlp_accuracy],\n",
        "    'F1 Score': [bert_f1, lstm_f1, mlp_f1]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_dict)\n",
        "comparison_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896332209
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mutation Scores\n",
        "print(mutation_score_1, mutation_score_2)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896332323
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Time (s)\n",
        "print(training_time)\n",
        "print(time_to_build_M)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722896332437
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}